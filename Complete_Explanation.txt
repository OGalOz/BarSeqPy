    TODO:
        
        Line 348 in FEBA.R - what is the whole rolling median thing?
        Why is Standard Deviation computed the way it is?
        Compare outputs from AvgStrainFitness between Python and R.
        Should users get the option of computing unedited (but still weighted)
        fitness values? Without the pseudocounts?
        Can we just remove all rows from all.poolcount that aren't inserted
        in genes? Why keep any more than we use? At what point do we stop
        using all the rows?



        Run . testPy.sh 4

        How do you explain the whole 'strain Pseudo Count' and strainFit Weight
               etc from analysis1 ?? 

        Why are lr and lrn exactly the same? (from BP4?)
        Why do they start so low down the number of genes?

        Get examples of intermediate variables in AvgStrainFitness, StrainPseudoCount, etc.
        strain_lr is not completely empty
        'normalize_per_strain_values' doesn't make sense (analysis2)
        strain_lrn is not returning any results
            Where is strain_lrn being computed?
                At function create_strain_lrn in analysis2

        specphe (specific phenotypes) and cofit not being computed (analysis3)
        No description for NormalizeByScaffold in analysis1

        Ask meaning of function:
        getStrainPseudoCount
    
    Vocabulary:
        exp_name: Experiment name, string, like 'set2IT008' the name of a specific experiment.
        SetName: Like 'set2', a more general collection of experiments, within which indexes 
                 define experiments.
        Date: The day on which an experiment may have occured
        Strain: associated with a row in all.poolcount, this refers to a unique mutation
                in the DNA of an organism, in that it had a transposon insertion.
                Each barcode and location is essentially a 'strain', and the number
                of times that strain appears is correlated with the number of
                reads found.

    Function Descriptions:
        Use 'Rapid Quantification of Mutant Fitness in 
        Diverse Bacteria by Sequencing Randomly Bar-Coded 
        Transposons' pages 12-14 as a reference
        create_strain_lrn (analysis2): 
            We take the strain log ratios, which are created in the function 
            'StrainFitness' from analysis1 and we normalize them by finding
            the median over all the values from the same scaffolds and
            subtracting that from the log ratios.
            We also take a dataframe that is the gene log ratios - gene
            normalized log ratios, and if those exist at a read, then
            we subtract those instead of the median.

        GeneFitness (analysis1):
            

        AvgStrainFitness (analysis1): Done for each experiment_name
            We take three main input variables:
                1. exp_used_strains: The used reads values from all_df, 
                    it is a pandas Series of values, whose index is the row number
                    from all.poolcount where it originated, and whose length is
                    nAllStrainsCentralGoodGenes*, and the * is there because
                    it could include strains that are in all the genes
                    with good insertions, or it could only be the reads
                    with even stronger gene insertions, where there are
                    enough reads in both the top and bottom half.
                2. t0_used: The sum of the t0 reads for this experiment,
                    taken from all_df, and summed over the columns whose
                    experiments are associated with the t0 value for
                    this experiment_name. This is also a pandas Series,
                    whose  indexes are exactly the same as for exp_used_strains.
                    length = nAllStrainsCentralGoodGenes*
                3. all_used_locId: The associated locusIds for the above
                    two pandas Series, has the same index. Also taken
                    from all.poolcount (all_df)
                    length = nAllStrainsCentralGoodGenes*
            This function is run 3 times. The first time we run it on
            all the strains who are inserted in a good place in genes
            that we are using with high enough abundance. main_df
            The second time we run it we are running it on the strains who are 
            inserted within 0.1 and 0.5 of the gene (0.1<f<0.5). df_1
            The third time we run it we are running it on the strains who are 
            inserted within 0.5 and 0.9 of the gene (0.5<f<0.9). df_2
            The next step is that we get the 'readratio', which is a float.
            The readratio is the sum of the reads for this experiment
            divided by the sum of the reads for the time0s for this
            experiment.
            Now we create a variable called 'strainFit', which is essentially
            taking the log2 values of exp_used_strains (the experiment original
            reads) and subtracting the log2 values of the time0s for those reads,
            and then doing a median normalization of the difference.
            So log2(originalreads) - log2(t0reads) = x; then return mednorm(x).
            BUT we add in the readratio to the original reads and the inverse
            of the readratio to the t0reads. Why?
            Now we create strainPseudoCount. In order to create strainPseudo count,
            first we get the medians of the strainFit values (already log2 normalized) 
            over the various genes. So suppose there are 15 strains (log2 normalized)
            in one gene (e.g. 'trp'); then we take the median of all those and essentially
            create a mapping from locusId -> median. This is called geneFitMedians.
            Then we also get a counting of how many times each gene was inserted into,
            so for that gene 'trp' we would get a mapping to the number 15. This 
            is done for all genes (represented by locusId strings) and is placed
            in a variable called locusId2TimesSeen_d.
            Now we create the strainPseudoCount list, whose length will be 
            nAllStrainsCentralGoodGenes(*), i.o.w as long as the three main
            inputs. How we create it is like this:
            We go through each locusId in locusId2TimesSeen_d, and check if the number
            of times it was seen is greater than a threshold integer called
            'minGeneFactorNStrains'. If it is greater than this threshold, we
            add the number 2^(median of the locusId (from geneFitMedians)) multiplied
            by the readratio, which is sum(reads)/sum(time0s). Which is essentially
            reversing the strainFit action, making strainPseudoCounts essentially
            the strain counts but over the medians.
            That being said, if the number of times the locusId was seen doesn't 
            pass the threshold, then we simply add the readratio itself to the
            strainPseudoCount list.
            Now we use the strainPseudoCount list to adjust the strainFit values.
            This part is unclear to me why it's done the way it is done.
            In order to create an adjusted StrainFit, we take the strainPseudoCount
            list, and take its square root (element-wise) and call it expPC,
            then we create a T0 pseudocount which is 1/expPC (call it t0PC). 
            Now we adjust the strainFit by adding these PseudoCount square root
            and inverse to their respective original reads values, taking 
            the log2 of those and subtracting the T0 sum from the Exp sum,
            and also subtracting a 'strainFitAdjust' number. But for this
            strainFit, which is adjusted, we don't apply median normalization
            for some reason (?).
            Now we compute the Standard Deviation per strain, (How does this 
            computation give the standard deviation?)
            Then we get weights per strain, based on the number of reads in the Time0
            and the experiments. The weights are used to measure strainFitness for
            a gene more accurately. The strains with less reads are weighted less,
            and the strains with more reads are weighted higher.
            Once we have the Standard Deviation per strain, the weights per strain,
            and the adjusted strain fitness, then we can go on to compute the following
            values per gene:
                fitRaw (float): Sum of weighted adjusted fitness scores divided by total weight. 
                sd (float): Standard Deviation computed fancy way
                sumsq (float): [Sum of the weighted square of the difference between adjusted fitness 
                                and fitRaw] divided by total weight.
                sdNaive (float): Standard Deviation computed in Naive way 
                n (int): Total number of strains in this gene
                nEff (float ): The sum of the strain weights in these indeces/ max weight
                tot (int ): The sum of the experiment reads over the locusID
                tot0 (int): The sum of the Time0s over the gene
            We create a dictionary which stores all of these values per gene, and then
            once we're done computing over all the locusIds, we convert the dictionary
            into a dataframe, with the columns being the above values (fitRaw,...,tot0).
            Then we create two new values which are based on the above values over all
            the genes: 'fit' and 'fitNaive'. 'fit' is the median normalization of 'fitRaw' from
            each gene, then 'fitNaive' ss a naive computation of fitness based on tot and
            tot0 (median normalized log2 difference of the two).
            We return this dataframe called 'fitness_df'.
       NormalizeByScaffold (analysis1):
          We create the column 'fitnorm' for main_df from GeneFitness.
          How:
          We take the series of locusIds from main_df (which are unique),
          and find their locations within the series genes_df['locusId'] which are also
          unique. Then we take the subset of the dataframe genes_df which match
          to those locations and take its scaffoldId, begin values and locusIds and
          combine them with the fitness values from AvgStrainFitness, and create
          a dataframe called tmp_df.
          Then we group values by scaffoldIds (which are not unique), and get
          the rows of tmp_df which are associated with that scaffoldId.
          We check that the total number of rows associated with that scaffoldId
          pass the threshold 'minToUse'. If it doesn't, we remove the fitness
          values for those rows. If it does we continue to the next part.
          Now we take the median of the rows associated with that scaffoldId
          and subtract it from them (median normalization on a subset of the
          rows).
          Then, if the number of rows associated with this scaffoldId pass
          the window threshold, we take the running median with window size
          251 of the sorted genes (A running median looks backwards and 
          forwards the same amount and computes the median using half 
          the window after and half before, so in this case 125 after and 
          125 before), and then we subtract that median from those values.
          Then we also normalize by the mode, and a simple way to 
          estimate the mode if there aren't repeating values is through
          the gaussian kernel density estimation function:
          //rmflight.github.io/post/finding-modes-using-kernel-density-estimates/

            
            



                    

            Within this function we create the following variables:
                strainWeight, strainFit_adjusted, strainSD.

                    strainWeight: comes from t0_used, and exp_used_strains
                    strainFit_adjusted: comes from t0_used, exp_used_strains,
                                        condPseudoCount, and t0PseudoCount.
                                        condPseudoCount and t0PseudoCount
                                        come from strainPseudoCount,
                                        which comes from geneFit1 and readratio,
                                        geneFit1 comes from strainFit,
                                        which is the simple computation
                                        you'd expect the program to do.
                                        



    Main Variables:
        strainsUsed:
            list<bool> Length of all_df which decides which of the 'strains'
            we actually use. Must pass two qualifications:
            The mean of the t0tot over the strain has to pass a threshold 
                'minT0Strain'
            The insertion location of the strain has to be between 0.1 and 0.9
            of a gene.

        What are main lengths and how do they relate to lengths of input variables?
        Main lengths:
            nAllStrains = number of rows ( all.poolcount )
            nAllStrainsCentral = number of rows in all.poolcount with 0.1<f<0.9
            nAllStrainsCentralGoodGenes = number of rows in all.poolcount with 0.1<f<0.9
                                          AND all locusIds are in the list 'GenesUsed'
                                            (27659 in Keio) - also known as nUsefulReads
            nAllStrainsCentralGoodGenes12 = number of rows in all.poolcount with 0.1<f<0.9
                                          AND all locusIds are in the list 'GenesUsed12'
                                            (27659 in Keio) - also known as nUsefulReads
            nAllStrainsCentralGoodGenes* = either of nAllStrainsCentralGoodGenes(12) 
            nTotalGenes = number of rows (genes.GC)
            nGenesUsed = (len(genesUsed)) number of rows in genes.GC that we actually use
                        which is equivalent to the number of unique genes
                        that have good insertions in them. (1355 in Keio)
                        Which is the same as the output fitness and t score dataframes
            nGenesUsed12 = (len(genesUsed12)) number of locusIds with a good amount of 
                            insertions in both halves of 'f' from all_df. Both df1 and df2 
                            in GeneFitness() have this number of rows.
            nGenesUsed* = either nGenesUsed or nGenesUsed12
            nExperiments = number of rows in experiments file
    

    Questions:
        Why are we normalizing by scaffold?
        There are issues with some outputs: fit_log_ratios_unnormalized_naive:
            The dataframe isn't combined, it's just two dataframes stacked on top of 
            one another.
        Should we allow people to get the raw results from their reads, the raw log2
        differences between values and time0s? Instead of all the normalization


    Outputs:

    The Standard output columns are:
            locusId, sysName, desc experiment_1, experiment_2, ..., experiment_n
            where experiment_1 etc refer to the setName + Index values
            These often have the same number of rows and contain numerical values under 
            the various experiments.

    Throughout the following, these column data types are fixed:
        locusId (str): The gene ID, also known as 'locus ID'
        desc (str): Short description of what a gene does
        name (str): Name of the experiment, or name of a gene 
        strand (str): Either '+' or '-', indicating positive or minus strand, respectively.
        begin (int): Where the object of interest begins within a scaffold
        end (int): Where the object of interest ends within a scaffold
        sysName (str): Another identifier for a gene


        fit_logratios_unnormalized_naive: The fitness logratios that are unnormalized
                                        Have Standard output columns
        cofit: Contains columns:
            locusId, sysName, desc, hitId, cofit, rank, hitSysName, hitDesc

        fit_logratios:
                                        Have Standard output columns

        fit_quality:
            Has the following columns:
                name, short, t0set, num, nMapped, nPastEnd, nGenic, nUsed, gMed, gMedt0, 
                gMean, cor12, mad12, mad12c, mad12c_t0, opcor, adjcor, gccor, maxFit, u

        fit_standard_error_naive:
                                        Have Standard output columns

        fit_standard_error_obs:
                                        Have Standard output columns

        fit_genes: (Complete) The genes.GC file with an extra column called 'used' which
                        contains boolean values for whether or not the gene was used.
                        Contains the following columns:
                            locusId (str):
                            sysName (str):	
                            type (int): Identifier for the type of gene
                            scaffoldId (str):	
                            begin (int):	
                            end (int):	
                            strand (str): '+'|'-'	
                            name (str): typical name for gene, e.g. "thrL"	
                            desc (str): What does the gene do?
                            GC (float):	Fraction of nucleotides that is GC
                            nTA (int):	number of TAs?
                            used (bool): gene used in analysis?

        fit_t:
                                        Have Standard output columns

        gene_counts:
                                        Have Standard output columns

                    Counts the number of times insertions occured in that gene in a given experiment.

        high_fitness: 
            Hybrid of a few of the other tables: Columns:
            locusId, expName, fit, t, se, sdNaive, name, Group, Condition_1, 
            Concentration_1, Units_1, Media, short, u, maxFit, gMean, sysName, desc

        specific_phenotypes:
            locusId, sysName, desc, short, Group, Condition_1, Concentraion_1, Units_1, Condition_2, Concentration_2, Units_2

        strain_fit:
            barcode, rcbarcode, scaffold, strand, pos, locusId, f, used, enoughT0, experiment_1   experiment_2  ... experiment_n
            
        strong:
            locusId, name, t, lrn, sysName, desc, short
        expsUsed:
           All the column names from the input experiments file + the following four:
                num (int) (counts which experiment, 
                short (str): either 'Time0' or a brief description of experiment conditions,
                name (str): the experiment name, e.g. set2IT003 
                t0set (str): A date which refers to the time0 to which we compare this one 
        

    
    Breakpoint Outputs Explained:
        BP3:
            gene_fit:
                Number of rows = length of genesUsed as input to analysis1

        GeneFitResults:
            For each experiment name, there are 3 different important variables:
               gene_fit, strain_fit, strain_se
            
            The length of gene_fit is nGenesUsed
            The length of strain_fit is  nAllStrainsCentralGoodGenes (nUsefulReads)





    Program is divided into 7 phases:
        1. Data Preparation 1: Naming and formatting (labeling)
        2. Data Preparation 2: Accounting for Controls (Known as "Time0")
                a) Finding and Summing Controls
                b) Finding which strains and genes are good to use and pass thresholds.
        3. Computations 1: Log Ratios, T-scores and Normalizations
        4. Computations 2: Correlations and meta-statistics
        5. Computations 3: Cofitness and finding high fitness values
        6. Exporting and Visualizing Graphs
        7. Website building


    Requirements:
        Inputs:
            all.poolcount: List of 'strains'. If a transposon was inserted inside a gene,
                then the field locusId will no longer be empty, and the field 'f' will no
                longer be empty, since 'f' measures the fraction of the gene in which
                the transposon was inserted, i.e. if it was inserted in the 25th% of
                the gene in terms of length, then its 'f' would be 0.25.
                For each identifier for an experiment, we count the number of times
                that strain appears.


        
       All the locusIds in all.poolcount have to exist in the genes file (genes.GC)

       Experiments File:
        If you want to drop an experiment (row in experiments file), you must create a column
         called 'Drop' and write in "true" (upper or lower cases don't matter). You can leave
         this column in the other experiments as nothing. You can also remove the row representing
         the experiment from the file entirely.
        The 'SetName' column contains the setname (lane) the experiments come from, 
        where as the 'Index' is more specifically the exact solution. 
        So the 'SetName' + 'Index' indicates a unique collection.

        Don't allow users to add 'strainsUsed'
        

    Random:

        Individual Experiments are denoted both by a row in the 'Experiments' file, and by a SetName.Index;
        Either one can be thought of as an individual experiment. Eventually, the "." in the SetName.Index is
        removed, and we are left with just SetNameIndex.
        SetNames can also be thought of as "lanes".
        How should dates be labelled? always X1/X2/X3..? 
        On any given date, a few experiments are Time0 experiments. So for example on  
            "6/19/2014" there were a few experiments started out of "Keio_ML9_set2", and we 
            can store all of those experiments in a dict.

        t0tot takes dates and sums all columns of all_df (that are experiments) over those dates.
        t0gN takes it one step further, and sums over the rows that have the same locusIds and that
            have central insertions.
        


    Input function is through RunFEBA.py:


        The 'readratio' is the ratio between the sum of all final values for a setIndex Name
            from all.poolcount and the sum of the corresponding t0 values.
            It is used in calculating the Strain Fitness.

        The experiments file has to follow very specific requirements:
            If you want to Drop an experiment, you need to create a column
            for all your experiments called "Drop", and within that column
            you must add the value "TRUE" (or "True").
            The column 'name' must eventually be equivalent to the indexes
                in all.poolcount
            The column 'Group' must show if it's Time0 by saying 'Time0' when it
                is indeed a time0 element

        You can also add a list of set-index names to ignore



    ScaffoldId vs Scaffold as column name?
   
    The total number of rows in the dataframes is equivalent to the numbers of
        unique locusIds in strainLocus[strainsUsed].


    gene_fit_d:

    g -> genes
    lr -> log ratios
    lrn -> log ratios normalized
    lrn1 -> log ratios normalized 1st half
    q -> quality
    u -> used
    se -> standard error
    tmp -> temporary


        Stores combined data over all set_index_names.
            *All series/dataframes ave the exact same number of rows.
        g: A list of locusIds
        lr: Some rows are completely empty (?), whenever one col has values, 
            the other also has values.
        lr1/ lr2: If two columns, if one has value, the other has the opposite value 

        At the end of the program, gene_fit_d should have keys:
            'g', 'lrRaw', 'sd', 'sumsq', 'sdNaive', 'n', 'nEff', 'tot', 'tot0', 'lr', 
            'lrNaive', 'lrn', 'lr1', 'lr2', 'lrn1', 'lrn2', 'tot1', 'tot1_0', 'tot2', 
            'tot2_0', 'pseudovar', 'se', 't', 'version', 'q', 'genesUsed', 'strainsUsed', 
            'genesUsed12', 'gN', 't0_gN', 'strains', 'strain_lr', 'strain_se', 'high' 
    
    fitQuality:
        This returns a matrix with individual stats per setindexname. So if there are 4 set index names,
        e.g. set2IT001, set2IT002, set2IT003, set2IT004; You would get a matrix with the columns:
            nUsed, gMed, gMedt0, gMean, cor12, mad12, mad12c, mad12c_t0, opcor, adjcor, gccor, maxFit
        and the rows of the 4 set index names.

    Revisions:
        all_gN isn't used but created early on and stored as a variable (Why?)
        experiments DataFrame is updated at random times, e.g. in FEBA_Fit it is 
                updated to have 'short' set to Time0 if group is Time0, which could
                have been done earlier in the program, e.g. in 'RunFEBA'. Also,
                the t0set is created at a random point.
